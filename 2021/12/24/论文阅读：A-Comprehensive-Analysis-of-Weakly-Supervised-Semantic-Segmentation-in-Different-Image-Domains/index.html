



<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="Hexo" href="http://example.com/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="Hexo" href="http://example.com/atom.xml" />
<link rel="alternate" type="application/json" title="Hexo" href="http://example.com/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="cv; deep learning; weakly-supervised semantic segmentation" />


<link rel="canonical" href="http://example.com/2021/12/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/">



  <title>
	论文阅读：A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in
	Different Image Domains |
Yume Shoka = Hexo</title>
<meta name="generator" content="Hexo 5.4.0"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">	论文阅读：A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in
	Different Image Domains
  </h1>
  
<div class="meta">
  <span class="item" title="Created: 2021-12-24 11:13:48">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">Posted on</span>
    <time itemprop="dateCreated datePublished" datetime="2021-12-24T11:13:48+08:00">2021-12-24</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="Toggle navigation bar">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">Yume Shoka</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipeuv80yoj20zk0m8kjl.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicm0fdw5cj20zk0m8hdt.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giph4wqtg4j20zk0m8x6p.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclhnx9glj20zk0m8npd.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipeu7txpzj20zk0m81kx.jpg"></li>
          <li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclga70tsj20zk0m84mr.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">Home</a></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="en">
  <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content=", ">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <blockquote>
<p>论文题目： A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains</p>
</blockquote>
<blockquote>
<p>论文下载： <span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMjYzLTAyMC0wMTM3My00">https://link.springer.com/article/10.1007/s11263-020-01373-4</span></p>
</blockquote>
<blockquote>
<p>源码地址： <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2x5bmRvbmNoYW4vd3Nzcy1hbmFseXNpcw==">https://github.com/lyndonchan/wsss-analysis</span></p>
</blockquote>
<p>这是发表在 IJCV2021 上的一篇关于弱监督语义分割的综述论文，本文详细的测试了近年来几种不同图像域（自然图像，组织病理切片，卫星图像）上 sota 的弱监督语义分割方法在其他图像域上的表现，并分析了不同图像域图像的特点以及如何选取合适的弱监督语义分割方法。本篇笔记本人只关注其中的图像级弱监督语义分割问题，对于其他方法有描述不当的地方欢迎路过的大佬指正。</p>
<h1 id="abstract"><a class="markdownIt-Anchor" href="#abstract">#</a> Abstract</h1>
<p>尽管只使用缺少位置信息的图像级标签进行训练，最近提出的弱监督语义分割方法还是取得了令人印象深刻的性能。由于图像级标签便宜且省时，弱监督的语义分割方法比全监督的方法更加实用。这些方法主要是为了解决自然场景图像的背景分离和部分分割问题的，目前尚不清楚它们是否可以简单的移植到其他图像领域并且仍能表现良好，如病理切片和卫星图像等。本论文评估了在自然图像、组织病理图像和卫星图像数据集上最先进的弱监督语义分割方法，并分析了如何确定哪种方法更适合某一给定的数据集。我们的实验表明，与自然场景的图像相比，组织病理学图像和卫星图像表现出一组不同的关于弱监督语义分割的问题比如模糊的边界和类共现问题等。</p>
<h1 id="introduction"><a class="markdownIt-Anchor" href="#introduction">#</a> Introduction</h1>
<p>Multi-class semantic segmentation 可以完成许多工作如作为注意力机制使模型只关注相关的区域；将像素转化为更高层（具有更多语义信息）的表示如物体定位，形状，大小，纹理（texture），姿势或动作等以促进对图像的进一步分析。自然图像中的语义分割可以监测交通流量，从图像中分割人或者收集人群统计数据等。组织病理图像中的语义分割可以监测异常形状的肾组织，量化细胞大小和密度，并可以构建基于组织的图像检索系统等。卫星图像中的语义分割可以检测农田里面的杂草，洪水区域以及量化城市发展等。</p>
<p>尽管全监督的语义分割方法性能令人振奋，其所需要的像素级标注过于耗时费力且昂贵。弱监督的语义分割方法使用比像素级标注信息量更少的标注完成语义分割任务，大大节省了人力时间成本，从而节省了金钱成本。据统计，MS COCO 数据集的标注者为一张图像做图像级标签平均需要 4.1 秒，而为一张图像做像素级标注则需要平均 10.1 分钟，是图像级标签的 150 倍。</p>
<p>定性的说，使用图像级标签的弱监督语义分割方法需要仅根据一个物体是否存在来确定其在图像中的位置，目前在自然图像上的弱监督语义分割方法只用了全监督语义分割方法一部分的标注却可以得到优秀的性能。由于图像级标签完全缺少位置信息，我们总结了目前的在自然图像上的弱监督语义分割方法的困难：</p>
<ol>
<li>从背景中分割出前景物体，尤其是背景中包含与前景目标有强共现性的物体时，如水和船，我们只知道图片中有船，但是大部分有船的图片都有水，因此只根据图像级标签训练的模型便无法准确地区分出船和水的边界。</li>
<li>区分强共现性物体。同上一点一样，如果两种物体频繁的共同出现，模型便很难区分出他们的边界。</li>
<li>分割出完整的物体而不是物体最值得注意（discriminative）的部分，比如我们需要分割出整个猫而不是仅仅分割出猫的头。众所周知，分类网络在优化 cross entropy loss 的时候只需要给最值得注意的区域（如猫的头）很高的激活值，该物体的其他区域完全可以不去管他，这就造成了模型很容易分割出一个物体最值得注意的部分却很难完整的分割出来整个物体。解决该问题的方法有：（1）使用具有更大感受野的模型。（2）使用擦除或空间区域 dropout 的方法使模型关注到最值得注意的区域以外的地方。（3）使用语义相似性信息将最值得注意的区域传播到物体的整个范围。</li>
</ol>
<p>自然场景图像和组织病理图像以及卫星图像的区别：</p>
<ol>
<li>自然场景图像包含更多粗粒度的视觉信息（即低类内差异和搞类间差异），而卫星图像和组织病理图像包含更多细粒度的对象（即高类内差异和高类间差异）。</li>
<li>组织病理图像和卫星图像的边界不明确，甚至专家在标注这些图像时也很难达成一致。</li>
<li>组织病理图像和卫星图像总是以相同的比例和视角成像，遮挡和光照变化最小。</li>
</ol>
<p>这些差异使得不能盲目地将自然图像的方法用于其他图像，甚至完全不同的方法在其他图像上可能表现更好。</p>
<h1 id="related-work"><a class="markdownIt-Anchor" href="#related-work">#</a> Related Work</h1>
<p>作者在这一部分介绍了三个图像域中具有代表性的数据集和弱监督语义分割方法，数据集部分我就不再赘述了，我们直接跳到方法部分。</p>
<h2 id="weakly-supervised-semantic-segmentation"><a class="markdownIt-Anchor" href="#weakly-supervised-semantic-segmentation">#</a> Weakly-Supervised Semantic Segmentation</h2>
<p>已经提出的弱监督语义分割方法主要可以分为以下四类：Expectation-Maximization, Multiple Instance Learning, Object Proposal Class Inference, and Self-Supervised Learning.<br>
 表 2 根绝各个方法的共同特症列出了各个方法：<br>
<img data-src="./table2.png" alt=""><br>
表 3 根据时间顺序列出了各个方法以及其代码可用性和在 PASCAL VOC2012 上的性能：<br>
<img data-src="./table3.png" alt=""></p>
<h3 id="expection-maximization"><a class="markdownIt-Anchor" href="#expection-maximization">#</a> Expection-Maximization</h3>
<p>这类方法我暂时还没有看懂，等我看懂了再补。</p>
<h3 id="multiple-instance-learning"><a class="markdownIt-Anchor" href="#multiple-instance-learning">#</a> Multiple Instance Learning</h3>
<p>通常的 Multi-class classification 一般指给定一个样本和其所属的类别作为标签训练一个分类模型模型，而以二分类为例解释 Multi Instance Learning 的做法则是：给定一堆样本，只要其中有一个阳性样本则这堆样本的标签就是阳性，如果所有的样本均为阴性样本则该堆样本的标签才是阴性，以此作为输入设计算法。</p>
<ul>
<li>MIL-FCN (2014): 该方法训练一个以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 卷积层和全局最大值池化层为 head 的模型，以图像级标签作为监督信号。在推理阶段则是预测每个位置处最可能的类别，然后将预测图上采样到原图大小。</li>
<li>DCSM (2016): 用图像级标签训练一个 CNN 然后利用 GBP (guided back-propagation) 在中上层（我也不清楚中上层指的是深层还是浅层的卷积层）的卷积层中获得类激活图，然后用这些图进行作差和平均的操作获得不同层和尺度的激活图，最后用 CRF 进行后处理。</li>
<li>BFBP (2016): 看不懂 (trains a FCN with a foreground/background mask generated by CRF on the scaled average of conv4 and conv5 features with cross-entropy loss between the image-level annotations and the LSE pool of foreground and background-masked features; CRF post-processing is applied at test time.)</li>
<li>WILDCAT (2017): 看不懂 (trains a FCN with conv5 features being fed into a WSL transfer network, then applies class-wise average pooling and weighted spatial average of top- and lowest-activating activations; at test time, it infers the maximum-scoring class per position and post-processes with CRF.)</li>
</ul>
<h3 id="object-proposal-class-inference"><a class="markdownIt-Anchor" href="#object-proposal-class-inference">#</a> Object Proposal Class Inference</h3>
<p>这些方法目前我还看不懂，有空再补。<br>
<img data-src="./Object_Proposal_Class_Inference1.png" alt=""><br>
<img data-src="./Object_Proposal_Class_Inference2.png" alt=""></p>
<h3 id="self-supervised-learning"><a class="markdownIt-Anchor" href="#self-supervised-learning">#</a> Self-supervised Learning</h3>
<p>这种方法与 Multi Instance Learning 的方法类似，即使用推理生成的像素级激活图作为伪标签进行自监督学习。在实际操作中，我们通常的做法是先训练一个”backbone“分类网络以生成 CAM seeds 然后使用 CAM seeds 训练 FCN 用于最终的分割任务。</p>
<ul>
<li>SEC (2016): 这是这类方法的一个典型代表，它训练一个 CNN 并将生成的 CAM 作为训练 FCN 的伪标签，训练 FCN 时的监督信号有图像级的标签，CRF 处理过的 CAM 伪标签。</li>
<li>MDC (2018): 其在训练 CNN 的时候使用了 multi-dilated convolutional (空洞卷积)，并将空洞卷积用于生成 CAM 的过程然后生成伪标签并和 class score-weighted maps 训练 FCN 网络。</li>
</ul>
<p>然而，以上两种方法存在的问题是：最终得到的分割结果只能很好的分出最值得注意的区域。其中一种方法是使用对抗或随机擦除（adversarial or stochastic erasing）的方法，从而使模型可以关注到最值得注意的区域以外的区域：</p>
<ul>
<li>AE-PSL (2017): 和 SEC 一样，该方法也是生成 CAM 作为伪标签训练 FCN，不同的是：在训练 CNN 的过程中，高激活分数的区域会被从训练图片中擦除。</li>
<li>FickleNet (2019): 利用图像级标签训练 CNN，，其在后面的卷积层中具有中心固定的空间维度的 dropout（通过丢弃卷积窗口中非中心的像素实现），然后多次使用 Grad-CAM 生成阈值化的伪标签以训练 FCN。</li>
</ul>
<p>另外一种解决方法是简单的将类激活从自信度较高的区域传播到相邻的具有相似视觉特征的区域中，从而使生成的伪标签能覆盖到目标物体更广的区域：</p>
<ul>
<li>DSRG (2018): 训练一个 CNN 并且对生成的 CAM 使用 region-growing 的方法后生成伪标签用于训练 FCN。</li>
<li>PSA (2018): 训练一个 CNN 并通过 random walk 的算法将 CAM 扩展到周围具有语义相似性的区域，从而生成伪标签用于 FCN 的训练。</li>
<li>IRNet (2019): 该方法也是类似的思路，只不过该方法通过从 CAM 中获得的低位移场质心实施 random walk 算法来分割类的实例，直到类的边界，从而生成伪标签来训练 FCN。</li>
</ul>
<p>值得一提的是：根绝在 PASCAL VOC 上的定量结果显示，前五的弱监督语义分割方法均是采用的 self-supervised learning 方法，并且其中三个还使用了将类激活传播的方法。</p>
<h1 id="method"><a class="markdownIt-Anchor" href="#method">#</a> Method</h1>
<h2 id="seed-expand-and-constrainsec"><a class="markdownIt-Anchor" href="#seed-expand-and-constrainsec">#</a> Seed, Expand and Constrain(SEC)</h2>
<p>该方法的训练包括四个阶段，其整体框架如下图：<br>
<img data-src="./SEC_frame_work.png" alt=""></p>
<ol>
<li>用图像级标签训练一个分类网络</li>
<li>从分类网络中提取 CAM</li>
<li>将 CAM 阈值化并解决区域重叠冲突后作为种子（seeds/cues）</li>
<li>用生成的种子（seeds）进行 FCN 的自监督训练</li>
</ol>
<p>四个阶段的细节如下：</p>
<ul>
<li>Classification CNN: 这里会用图像级标签训练两个分类网络：
<ul>
<li>foreground network: VGG16 的变体，即将最后的两个 pooling 层和全连接层换成 GAP 和一个全连接层</li>
<li>background network: VGG16 的变体，即省略了最后两个卷积模块</li>
</ul>
</li>
<li>CAM: 从 foreground network 和 background network 中分别提取 CAM</li>
<li>Seed Generation: 对于 foreground network，其将最大激活值的 20% 作为弱的位置线索（weak localization cue）；对于 background network，添加背景类 CAM，应用 2D 中值滤波器，并将 10% 最低激活像素作为附加背景线索。 在线索重叠的区域，具有较小线索的类别优先。</li>
<li>Self-Supervised FCN Learning: 训练的 FCN 网络有三个损失函数作为监督信号：（1）生成的 weak localization cues（2）图像级标签（3）将最终生成的 mask 最为输入传入 CRF 来作为最终分割结果的监督。</li>
</ul>
<p>值得一提的是，该方法使用 denseCRF 最为最终结果的后处理手段。</p>
<h2 id="deep-seeded-region-growingdsrg"><a class="markdownIt-Anchor" href="#deep-seeded-region-growingdsrg">#</a> Deep Seeded Region Growing(DSRG)</h2>
<p>该方法的整体框架如下图所示：<br>
<img data-src="./DSRG_framework.png" alt=""><br>
该方法与 SEC 主要有一下不同：</p>
<ol>
<li>该方法不使用 background network 生成背景类的 activation map，而是使用一种现成的方法 DRFI 生成。</li>
<li>提取 CAMs 并以最大激活值的 20% 为阈值进行阈值化然后用卷积特征作为 region growing 的依据生成伪标签。</li>
<li>训练 FCN 的时候使用两个损失函数：（1）用第 2 步生成的伪标签作为监督信号。（2）将最终生成结果送入 dense CRF 后监督自身以学习更好的边界。</li>
</ol>
<p>该方法在推理阶段也使用 dense CRF 作为后处理手段。</p>
<h2 id="inter-pixel-relation-networkirnet"><a class="markdownIt-Anchor" href="#inter-pixel-relation-networkirnet">#</a> Inter-pixel Relation Network(IRNet)</h2>
<p>该方法的整体框架图如下所示：<br>
<img data-src="./IRNet_framework.png" alt=""><br>
该方法和 SEC 以及 DSRG 一样会使用 CAM 作为伪标签，但是不同的是该方法在 backbone 后设置两个 head 预测辅助信息而非直接预测最终的 mask。</p>
<ul>
<li>Classification CNN &amp; CAM: 与 SEC 和 DSRG 类似，该方法训练一个分类网络（ResNet50）并从中提取 CAM。</li>
<li>Seed Generation: 对于前景类，阈值设置为 0.3 并将其经过 denseCRF 处理后作为前景类的 seed，对于背景类，如果该区域没有前景的种子并且其激活值小于 0.05 则将其认为是背景区域，经过 denseCRF 处理后作为背景类的 seed</li>
<li>Self-Supervised DF and CBM Learning: 上一步生成的前景和背景 seeds 在这一步中被作为伪标签训练 backbone 后面的两个分支：
<ul>
<li>displacement field（DF，位移场），该分支用来预测每个位置对于 seeds 实例中心的位移</li>
<li>class boundary map（CBM），该分支用来预测一个像素处是类边界的可能性（通过最大化不同类之间的像素处的值和最小化相同类之间的像素的值来获得）</li>
</ul>
</li>
<li>CAM Random Walk Propagation with CBM: 最后通过 random walk 算法将每一个类的预测区域传播到边界，CBM 的逆作为转移概率矩阵。</li>
</ul>
<h2 id="histosegnet"><a class="markdownIt-Anchor" href="#histosegnet">#</a> HistoSegNet</h2>
<p>该方法的整体框架图如下所示：<br>
<img data-src="./HistoSegNet_framework.png" alt=""></p>
<p>该方法训练阶段主要包括四个阶段：</p>
<ol>
<li>用 patch-level 的标注训练一个分类网络</li>
<li>一个手工设计的 Grad-CAM</li>
<li>激活图（activation map）调整，如 background /other 的激活值，class suntraction 等</li>
<li>denseCRF</li>
</ol>
<p>各个阶段的细节如下：</p>
<ul>
<li>
<p>Classification CNN: 用 HTT-label 训练一个分类网络，该网络是 VGG16 的变体：（1）将 softmax 层换为 sigmoid，（2）在每一个卷积层后面都加入 batchnormalization 层。（3）将 flatten 层换为 GMP（Global Max Pooling，全局最大值池化层）</p>
</li>
<li>
<p>Grad-CAM: 生成 Grad-CAM 后将其用 HTT 的 confidence 加权</p>
</li>
<li>
<p>Inter-HTT Adjustments: 该步骤就是对得到的 CAM 做一系列我看不懂的处理使其可以作为下一阶段的伪标签</p>
</li>
<li>
<p>Dense CRF: 对最终生成的预测图使用 Dense CRF 算法</p>
</li>
<li>
<p>Ablative Study<br>
SEC 和 DSRG 使用 VGG16 来生成它们的定位种子，HistoSegNet 使用一个更浅的 3 个 block 的 VGG16 变体。我们不禁有这样一个疑问：“分类网络的网络结构对弱监督语义分割的性能是否有影响？” 这个问题在以前从来没有被探讨过，今天我们使用 8 种 VGG16 的变体来分析一下不同网络结构对 HisoSegNet 的影响。我们使用的八种结构分别称为 M1，M2，M3，M4，M5，M6，M7 和 X1.7，如下图所示：<br>
<img data-src="./8_architectures.png" alt=""><br>
其中 M1 到 M4 分析网络深度对模型性能的影响，他们都使用 GAP 和一个全连接层，但是卷积模块的数量分别是 5，4,3,2 个；M5 到 M7 分析向量操作的影响，他们都有 3 个卷积模块和一个全连接层，他们向量化的操作分别是 GAP，Flatten，GMP；最后，X1.7 分析 HBR 的影响：它和 M7 的结构相同但是在 51 个 ADP 的类上训练但只在 31 个分割类上测试。8 个结构全都在 Keras 上训练 80 个 epoch 使用 cycical learning rate 的学习率调整方法，batch size 为 16，分别测试分类和分割的性能。</p>
</li>
</ul>
<p>分类的性能如下所示：<br>
<img data-src="./classification_performance.png" alt=""><br>
可以看出：更深的网络其分类性能更好，GMP 的效果比 GAP 和 Flatten 更好，另外，不使用 HBR 比使用 HBR 的性能更好。</p>
<p>分割的性能如下：<br>
<img data-src="./segmentation_performance.png" alt=""><br>
可以看出：对于形态学类型，使用 GMP 且不使用 HBR 的网络性能仍是最好，但对于网络深度而言，3 个 block 的时候分割的性能更好；对于功能类型，3 个 block 的网络性能仍然很好但是 Flatten 加 HBR 的效果却是最好的。</p>
<p>以上分析说明：分类网络的设计对后续的分割性能效果影响很大，更深的网络如 VGG16 在分类任务上的性能很好但对于和后续的分割性能却不是最好因为最终的 feature map 的分辨率很低。</p>
<h1 id="performance-evaluation"><a class="markdownIt-Anchor" href="#performance-evaluation">#</a> Performance Evaluation</h1>
<h2 id="adp"><a class="markdownIt-Anchor" href="#adp">#</a> ADP</h2>
<h3 id="quantitative-performance"><a class="markdownIt-Anchor" href="#quantitative-performance">#</a> Quantitative Performance</h3>
<p><img data-src="./ADP_%E5%AE%9A%E9%87%8F.png" alt=""><br>
可以看出:</p>
<ol>
<li>只有 HistoSegNet 的方法可以稳定的超过 Baseline Grad-CAM 的性能</li>
<li>对于该任务，X1.7 的网络结构比 VGG16 的要好</li>
<li>SEC 表现最遭，DSRG 稍微好一点儿，IRNet 的表现与 Baseline Grad-CAM 相当</li>
</ol>
<h3 id="qualitative-performance"><a class="markdownIt-Anchor" href="#qualitative-performance">#</a> Qualitative Performance</h3>
<p><img data-src="./ADP_%E5%AE%9A%E6%80%A7.png" alt=""><br>
可以看出，对于形态学类型而言，X1.7 的网络结构比 VGG16 的效果更好（我的理解是 X1.7 的 feature map 分辨率更大，可以更好的对应小目标）。尽管 SEC 和 DSRG 可以更好的对应物体轮廓，他们却也会过分夸大物体大小。</p>
<h2 id="pascal-voc2012"><a class="markdownIt-Anchor" href="#pascal-voc2012">#</a> PASCAL VOC2012</h2>
<h3 id="quantitative-performance-2"><a class="markdownIt-Anchor" href="#quantitative-performance-2">#</a> Quantitative Performance</h3>
<p><img data-src="./VOC_%E5%AE%9A%E9%87%8F.png" alt=""><br>
可以看出：</p>
<ol>
<li>只有 SEC 和 DSRG 可以稳定的超过 Baseline</li>
<li>对于该任务，VGG16 的结构整体上要优于 M7</li>
<li>使用 M7 的 SEC 效果最好</li>
</ol>
<h3 id="qualitative-performance-2"><a class="markdownIt-Anchor" href="#qualitative-performance-2">#</a> Qualitative Performance</h3>
<p><img data-src="./VOC_%E5%AE%9A%E6%80%A7.png" alt=""><br>
可以看出：</p>
<ol>
<li>VGG16 Grad-CAM 更能捕捉到完整的物体</li>
<li>SEC 和 DSRG 可以修正原始 Grad-CAM 中的错误（可能是因为 seeding loss 比较适合这个数据集），但是 HistoSegNet 却总是将 Grad-CAM 中的分割结果联系到错误的类别。</li>
<li>图三中 VGG16 HistoSegNet 将桌子和人混淆了，而 M7 HistoSegNet 只分出了人的头和胳膊。</li>
<li>所有的方法都很难分出经常共同出现的类别如图 6 中的船和水</li>
</ol>
<h2 id="deepglobe-land-cover-classification"><a class="markdownIt-Anchor" href="#deepglobe-land-cover-classification">#</a> DeepGlobe Land Cover Classification</h2>
<p>懒得整理卫星图像的结果了，把定量和定性结果贴出来供大家参考。<br>
<img data-src="./DeepGloble_%E5%AE%9A%E9%87%8F.png" alt=""><br>
<img data-src="./DeepGloble_%E5%AE%9A%E6%80%A7.png" alt=""></p>
<h1 id="analysis"><a class="markdownIt-Anchor" href="#analysis">#</a> Analysis</h1>
<h2 id="effect-of-classification-net-cue-sparseness"><a class="markdownIt-Anchor" href="#effect-of-classification-net-cue-sparseness">#</a> Effect of Classification Net Cue Sparseness</h2>
<p>经分析可知，对于数据集中单张图片的平均目标物体数量较多，则产生稀疏 cue 的分类网络效果较好，因为这种网络的 feature map 分辨率较高需要更少的上采样；而如果数据集中单张图片的平均目标数量较少则相反。</p>
<h2 id="is-self-supervised-learning-beneficial"><a class="markdownIt-Anchor" href="#is-self-supervised-learning-beneficial">#</a> Is Self-supervised Learning Beneficial?</h2>
<p>目前主流的弱监督学习方法是先训练分类网络生成弱的定位信息如 CAM 和 Grad-CAM，然后再用这些信息进行自监督学习训练一个 FCN。但是自监督学习是否对于所有图片域的图片都有效？观察结果是：自监督学习是否有帮助与阈值化的 Grad-CAM 与 ground truth 的覆盖程度有关。如下图所示：<br>
<img data-src="./recall.png" alt=""><br>
Grad-CAM 对 PASCAL VOC 中羊的覆盖较小，后续使用自监督学习的效果更好，Grad-CAM 对 ADP 中的图片覆盖程度很好，不使用自监督学习的 HistoSegNet 的效果更好。<br>
用召回率来衡量 Grad-CAM 对 ground truth 的覆盖程度的话，当召回率小于 40% 的时候，使用自监督学习的效果更好，当召回率大于 40% 的时候，使用非自监督学习的效果更好。这很直观，因为 CAM 和 Grad-CAM 因只能分割 VOC2012 中目标物体的一部分而臭名昭著，因此开发了带有损失函数的自监督方法，以通过奖励真阳性而不是惩罚假阳性来鼓励模型从小的种子中预测出完整的区域。虽然对于种子的召回率很低的情况下，这种策略有效，但是当种子覆盖大部分真实情况时，这显然是有害的。另外，通过减小阈值来提高召回率也不能增加自监督学习模型的性能。</p>
<h2 id="addressing-high-class-co-occurrence"><a class="markdownIt-Anchor" href="#addressing-high-class-co-occurrence">#</a> Addressing High Class Co-occurrence</h2>
<p>图像级标签的弱监督语义分割是所有弱监督语义分割中标签信息量最少的一种，因为模型需要仅根据某物体是否在图片中来推测出该物体的位置、形状、大小等信息。而类共现性无疑是一个很大的挑战，因为如果训练数据中两个类别总是一起出现，那模型便很难区分出两个物体。作者在论文中通过简单的丢弃图片中所含目标物体类别较多的图片，证明了这种 class balance 的方法可以提高共现性很高的类的性能。但是这样丢弃了大多数训练数据之后还是造成了模型的性能下降，因此需要提出更合理的 class balance 的方法来提高模型的鲁棒性。</p>
<h1 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion">#</a> Conclusion</h1>
<p>我们论文的发现清楚地表明，主流方法不适合这些其他图像领域。我们认为需要做更多的工作来开发 WSSS 的替代方法，这些方法要么专门用于这些图像域，要么至少可以推广到它们。** 与其目前专注于提高激活图种子的召回率，不如设计新的损失函数来细化模糊的激活图边界或解决高级共现问题，** 这将是未来组织病理学和卫星图像探索的更好方向。从替代图像域中正确分割图像的最先进算法，作者认为必须做更多的工作来开发能够泛化到不同图像域的新方法。</p>
<h1 id="我个人的感想"><a class="markdownIt-Anchor" href="#我个人的感想">#</a> 我个人的感想</h1>
<p>本文详细的比较了在自然图像和组织病理学图像上 SOTA 的几个方法在其他图像域上的表现，作者总结了弱监督语义分割的难点：</p>
<ol>
<li>从背景中分割出前景</li>
<li>类共现问题</li>
<li>分割出完整的物体</li>
</ol>
<p>作者分析了以下几种因素对分割性能的影响：</p>
<ol>
<li>分类网络生成 cue 的稀疏性</li>
<li>是否使用自监督学习</li>
<li>类共现性的影响</li>
</ol>
<p>作者在最后总结的时候还说到：与其目前专注于提高激活图种子的召回率，不如设计新的损失函数来细化模糊的激活图边界或解决高级共现问题。</p>

      <div class="tags">
          <a href="/tags/cv-deep-learning-weakly-supervised-semantic-segmentation/" rel="tag"><i class="ic i-tag"></i> cv; deep learning; weakly-supervised semantic segmentation</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">Edited on</span>
    <time title="Modified: 2021-12-26 23:07:18" itemprop="dateModified" datetime="2021-12-26T23:07:18+08:00">2021-12-26</time>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> Donate</button>
  <p>Give me a cup of [coffee]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="/images/wechatpay.png" alt="John Doe WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div>
        <img data-src="/images/alipay.png" alt="John Doe Alipay">
        <p>Alipay</p>
      </div>
      
      <div>
        <img data-src="/images/paypal.png" alt="John Doe PayPal">
        <p>PayPal</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>Post author:  </strong>John Doe <i class="ic i-at"><em>@</em></i>Hexo
  </li>
  <li class="link">
    <strong>Post link: </strong>
    <a href="http://example.com/2021/12/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/" title="	论文阅读：A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in
	Different Image Domains">http://example.com/2021/12/24/论文阅读：A-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/</a>
  </li>
  <li class="license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2021/12/21/hello-world/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclh0m9pdj20zk0m8hdt.jpg" title="Hello World">
  <span class="type">Previous Post</span>
  <span class="category"><i class="ic i-flag"></i> </span>
  <h3>Hello World</h3>
  </a>

    </div>
    <div class="item right">
    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="Contents">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#abstract"><span class="toc-number">1.</span> <span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">2.</span> <span class="toc-text"> Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#related-work"><span class="toc-number">3.</span> <span class="toc-text"> Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#weakly-supervised-semantic-segmentation"><span class="toc-number">3.1.</span> <span class="toc-text"> Weakly-Supervised Semantic Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#expection-maximization"><span class="toc-number">3.1.1.</span> <span class="toc-text"> Expection-Maximization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multiple-instance-learning"><span class="toc-number">3.1.2.</span> <span class="toc-text"> Multiple Instance Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#object-proposal-class-inference"><span class="toc-number">3.1.3.</span> <span class="toc-text"> Object Proposal Class Inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-supervised-learning"><span class="toc-number">3.1.4.</span> <span class="toc-text"> Self-supervised Learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#method"><span class="toc-number">4.</span> <span class="toc-text"> Method</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#seed-expand-and-constrainsec"><span class="toc-number">4.1.</span> <span class="toc-text"> Seed, Expand and Constrain(SEC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deep-seeded-region-growingdsrg"><span class="toc-number">4.2.</span> <span class="toc-text"> Deep Seeded Region Growing(DSRG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inter-pixel-relation-networkirnet"><span class="toc-number">4.3.</span> <span class="toc-text"> Inter-pixel Relation Network(IRNet)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#histosegnet"><span class="toc-number">4.4.</span> <span class="toc-text"> HistoSegNet</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#performance-evaluation"><span class="toc-number">5.</span> <span class="toc-text"> Performance Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#adp"><span class="toc-number">5.1.</span> <span class="toc-text"> ADP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#quantitative-performance"><span class="toc-number">5.1.1.</span> <span class="toc-text"> Quantitative Performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#qualitative-performance"><span class="toc-number">5.1.2.</span> <span class="toc-text"> Qualitative Performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pascal-voc2012"><span class="toc-number">5.2.</span> <span class="toc-text"> PASCAL VOC2012</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#quantitative-performance-2"><span class="toc-number">5.2.1.</span> <span class="toc-text"> Quantitative Performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#qualitative-performance-2"><span class="toc-number">5.2.2.</span> <span class="toc-text"> Qualitative Performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deepglobe-land-cover-classification"><span class="toc-number">5.3.</span> <span class="toc-text"> DeepGlobe Land Cover Classification</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#analysis"><span class="toc-number">6.</span> <span class="toc-text"> Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#effect-of-classification-net-cue-sparseness"><span class="toc-number">6.1.</span> <span class="toc-text"> Effect of Classification Net Cue Sparseness</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#is-self-supervised-learning-beneficial"><span class="toc-number">6.2.</span> <span class="toc-text"> Is Self-supervised Learning Beneficial?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#addressing-high-class-co-occurrence"><span class="toc-number">6.3.</span> <span class="toc-text"> Addressing High Class Co-occurrence</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#conclusion"><span class="toc-number">7.</span> <span class="toc-text"> Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%88%91%E4%B8%AA%E4%BA%BA%E7%9A%84%E6%84%9F%E6%83%B3"><span class="toc-number">8.</span> <span class="toc-text"> 我个人的感想</span></a></li></ol>
      </div>
      <div class="related panel pjax" data-title="Related">
      </div>
      <div class="overview panel" data-title="Overview">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="John Doe"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">John Doe</p>
  <div class="description" itemprop="description"></div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">2</span>
        <span class="name">posts</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">1</span>
        <span class="name">tags</span>
      </a>
    </div>
</nav>

<div class="social">
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>Home</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>Random Posts</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2021/12/21/hello-world/" title="Hello World">Hello World</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="/2021/12/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/" title="	论文阅读：A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in
	Different Image Domains">	论文阅读：A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in
	Different Image Domains</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>Recent Comments</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2021</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">John Doe @ Yume Shoka</span>
  </div>
  <div class="powered-by">
    Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2021/12/24/论文阅读：A-Comprehensive-Analysis-of-Weakly-Supervised-Semantic-Segmentation-in-Different-Image-Domains/',
    favicon: {
      show: "（●´3｀●）Goooood",
      hide: "(´Д｀)Booooom"
    },
    search : {
      placeholder: "Search for Posts",
      empty: "We didn't find any results for the search: ${query}",
      stats: "${hits} results found in ${time} ms"
    },
    valine: true,copy_tex: true,
    katex: true,fancybox: true,
    copyright: 'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
